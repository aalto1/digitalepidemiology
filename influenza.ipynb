{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"Thing to be done:\n",
    "1 correlation\n",
    "2 correlation with other pages\n",
    "3 predictive model - neural network\n",
    "\n",
    "\n",
    "1 compute another graph\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Real-Time Flu Estimation via Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the following notebook we are going to reproduce for Italy McIver paper et al. \"Wikipedia usage estimates Prevalence of Influenza-like illness in theUnited States in near real time\". In this paper we show a method of estimating, in near-real time, the level of influenza-like illness (ILI) in Italy by monitoring the rate of particular Wikipedia article views on a daily basis. We calculated on a weekly base the number of times certain influenza- or health-related Wikipedia articles were accessed and compared these data to one of the Italian health protection agency program called \"Influnet\".\n",
    "\n",
    "The Notebook in the those three following sections:\n",
    "\n",
    "* **Comparing Influnet and Wikipedia's Influenza click throught rate**\n",
    "    * Retrivial and cleaning of the wikipedia pages:\n",
    "        * Using 3rd party toolkit wikishark\n",
    "        * Downloading the raw data from https://dumps.wikimedia.org/ (bonus point)\n",
    "    * Plot the two scaled curves on the same figure\n",
    "    * Compute Correlation\n",
    "* **Comparing Influnet data with other Wikipedia's related pages:**\n",
    "    * Retrivial and Cleaning\n",
    "    * Plotting and correlation analysis among each other\n",
    "* **Estimate Flu outbreaks:**\n",
    "    * Lasso\n",
    "    * Jesus\n",
    "    \n",
    "In the whole notebook we will make use of the following convetions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Comparison between Influnet and  Wikipedia Influenza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define the auxiliary functions that I defined in order to perform in order to perform data analysis in a more efficient way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr \n",
    "import seaborn as sb\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def importInflunet(path):\n",
    "    '''\n",
    "    Reads the Influnet data and creates a unique multiindex dataframe of the format\n",
    "    \n",
    "    (year,week) - incidence\n",
    "    \n",
    "    :param path: location of the influnet folder\n",
    "    :return: compacted version of \n",
    "    '''\n",
    "    parser = lambda d: dt.datetime.strptime(d + '-0', \"%Y-%W-%w\")\n",
    "    \n",
    "    df = pd.concat([pd.read_csv(path+t,\n",
    "            names=[\"time\",\"incidence\"], sep=\" \", parse_dates = [\"time\"], date_parser = parser, \n",
    "        header=1, usecols=[0,4], decimal=\",\") for t in listdir(path)])\n",
    "    \n",
    "    df = df.set_index([\"time\"], append=False)\n",
    "    df[\"incidence\"] = df[\"incidence\"].astype(float)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    df = df.groupby(df.index).sum()\n",
    "    alpha = df.index[0]\n",
    "    omega = df.index[-1]\n",
    "    time_range = pd.date_range(alpha, omega, freq='W-SUN')\n",
    "    df = df.reindex(time_range, fill_value=0)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWikiRaw(wikiPages, path = \"/home/aalto/PycharmProjects/digitalepidemiology/data/\"):\n",
    "    '''\n",
    "    lplp\n",
    "    :param wikiPages: list of the wikipages that we want to analyze\n",
    "    :param path: location of the downloaded wikipedia pages\n",
    "    :return: \n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for wikiPage in wikiPages:\n",
    "        wiki = pd.read_csv(path+wikiPage+\".csv\", usecols=[0,1], parse_dates=True, index_col=[0], header=None)\n",
    "        wiki = wiki.resample(\"W-Sun\").sum()\n",
    "        df = df.reindex(wiki.index)\n",
    "        df[wikiPage] = wiki\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparePlots(elems, start):\n",
    "    for elem in elems:\n",
    "        y = elem.ix[elem.index.year > start, 0]\n",
    "        y = y/max(y)\n",
    "        plt.plot(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influnet = importInflunet(\"/home/aalto/Desktop/DE/hw2/influnet/data/\")\n",
    "wiki = getWiki([\"influenza2\"])\n",
    "\n",
    "comparePlots([influnet, wiki], 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          2010      2011      2012      2013      2014      2015      2016\n",
      "2010  0.515360  0.734056  0.788339  0.667596  0.679576  0.616321  0.306227\n",
      "2011  0.308532  0.905462  0.820030  0.912294  0.821094  0.914130  0.504102\n",
      "2012  0.316096  0.858859  0.809108  0.877478  0.796107  0.887290  0.505945\n",
      "2013  0.263040  0.764573  0.745169  0.826106  0.778834  0.826799  0.547922\n",
      "2014  0.307226  0.822641  0.784128  0.855148  0.816775  0.850329  0.543867\n",
      "2015  0.264612  0.861972  0.784301  0.883366  0.824500  0.903945  0.551272\n",
      "2016  0.497960  0.311634  0.548219  0.508097  0.516761  0.449457  0.823771\n"
     ]
    }
   ],
   "source": [
    "getCrossCorrelation([influnet, wiki], range(2010,2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need the wikipedia pages views count in order to compare them with influnet. In order to satisy the requirement for the bonus point I wrote a simple script that downloads the files from https://dumps.wikimedia.org/, scan it to find the words we are intrested in and writes those entries of the files that we are intrested and write them on different files. \n",
    "\n",
    "Once those files are collected and stored on our disk we can call this function which loads the element passed them in memoery and group them by week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the previous point we load the influnet dataset in memory and than show its plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 - Other functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are intrested to find if there is any other wikipedia pages that are able to gives us a better insghit about the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWikis(path = \"/home/aalto/PycharmProjects/digitalepidemiology/data/influenza1.csv\"):\n",
    "    '''\n",
    "    lplp\n",
    "    :param wikiPages: list of the wikipages that we want to analyze\n",
    "    :param path: location of the downloaded wikipedia pages\n",
    "    :return: \n",
    "    '''\n",
    "    df = pd.read_csv(path, parse_dates=['Date'])\n",
    "    df = df.set_index([\"Date\"], append=False)\n",
    "    del df[\"Week Number \"]\n",
    "    new_columns = dict((column,column[:-4].lower()) for column in df.columns.values)\n",
    "    df = df.rename(columns = new_columns )\n",
    "    del df[\"unname\"]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCrossCorrelation(elems, years):\n",
    "    '''\n",
    "    :param elems: \n",
    "    :param years: \n",
    "    :return: \n",
    "    '''\n",
    "    y = len(years)\n",
    "    heatmap = pd.DataFrame(np.zeros(y**2).reshape(y,y), index = years, columns=years)\n",
    "    \n",
    "    for y1 in years:\n",
    "        for y2 in years:\n",
    "            #print(\"ciao\",elems[0][elems[0].index.year == year1])\n",
    "            a = elems[0][elems[0].index.year == y1]\n",
    "            a = a[\"incidence\"]/max(a[\"incidence\"])\n",
    "            b = elems[1][elems[1].index.year == y2]\n",
    "            b = b/max(b)\n",
    "            minimum = min(len(a), len(b))\n",
    "            heatmap.ix[y1,y2] = pearsonr(a[:minimum].values, b[:minimum].values)[0]\n",
    "    print heatmap\n",
    "    sb.heatmap(heatmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influnet = importInflunet(\"/home/aalto/Desktop/DE/hw2/influnet/data/\")\n",
    "wikis = getWikis()\n",
    "for wiki in wikis:\n",
    "    #getCrossCorrelation([influnet, wikis[wiki]], range(2010,2017))\n",
    "    plt.plot(influnet/max(influnet[\"incidence\"]))\n",
    "    plt.plot(wikis[wiki]/max(wikis[wiki]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padInflunet(aux, year):\n",
    "    '''\n",
    "    The influnet dataset lacks information about the weeks that do not belog to the flu season (usally, but not necessarly, from week 17 to 40).\n",
    "    This functions fills the dataset with empty position in order to match the wikipedia format.\n",
    "    \n",
    "    :param aux: Influnet dataframe from a specific year\n",
    "    :param year: year of the previous Influnet dataframe\n",
    "    :return: padded version of the original dataframe\n",
    "    '''\n",
    "    year_weeks = aux.index.values[-1]\n",
    "    week_range = range(1,year_weeks+1)\n",
    "    aux = aux.reindex(week_range, fill_value=0)\n",
    "    aux[\"year\"] = year\n",
    "    aux[\"week\"] = week_range\n",
    "    \n",
    "    aux.set_index(['year', 'week'], append=False, inplace=True)\n",
    "    return aux\n",
    "\n",
    "\n",
    "def getInflunet(path = \"/home/aalto/Desktop/DE/hw2/influnet/data/\"):\n",
    "    '''\n",
    "    import and reformat the original Influnet dataset\n",
    "    \n",
    "    :param path: \n",
    "    :return: clean and padded version of the Influnet dataset\n",
    "    '''\n",
    "    \n",
    "    df = importInflunet();\n",
    "    previous = None\n",
    "    for x,y in df.index.values:\n",
    "        if previous == None:\n",
    "            df2 = reindexDF(df.loc[x], x)\n",
    "        elif x != previous:\n",
    "            df2 = df2.append(reindexDF(df.loc[x], x))\n",
    "        previous = x\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
