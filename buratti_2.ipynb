{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\"Thing to be done:\n",
    "1 correlation\n",
    "2 correlation with other pages\n",
    "3 predictive model - neural network\n",
    "\n",
    "\n",
    "1 compute another graph\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Real-Time Flu Estimation via Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the following notebook we are going to reproduce for Italy McIver paper et al. \"Wikipedia usage estimates Prevalence of Influenza-like illness in theUnited States in near real time\". In this paper we show a method of estimating, in near-real time, the level of influenza-like illness (ILI) in Italy by monitoring the rate of particular Wikipedia article views on a daily basis. We calculated on a weekly base the number of times certain influenza- or health-related Wikipedia articles were accessed and compared these data to one of the Italian health protection agency program called \"Influnet\".\n",
    "\n",
    "The Notebook in the those three following sections:\n",
    "\n",
    "* **Comparing Influnet and Wikipedia's Influenza click throught rate**\n",
    "    * Retrivial and cleaning of the wikipedia pages:\n",
    "        * Using 3rd party toolkit wikishark\n",
    "        * Downloading the raw data from https://dumps.wikimedia.org/ (bonus point)\n",
    "    * Plot the two scaled curves on the same figure\n",
    "    * Compute Correlation\n",
    "* **Comparing Influnet data with other Wikipedia's related pages:**\n",
    "    * Retrivial and Cleaning\n",
    "    * Plotting and correlation analysis among each other\n",
    "* **Estimate Flu outbreaks:**\n",
    "    * Lasso\n",
    "    * Jesus\n",
    "    \n",
    "In the whole notebook we will make use of the following convetions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Comparison between Influnet and  Wikipedia Influenza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to define the auxiliary functions that I defined in order to perform in order to perform data analysis in a more efficient way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr \n",
    "import seaborn as sb\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            incidence\n",
      "2003-10-28       0.47\n",
      "2003-11-04       0.52\n",
      "2003-11-11       0.60\n",
      "2003-11-18       0.59\n",
      "2003-11-25       0.70\n",
      "2003-12-02       0.64\n",
      "2003-12-09       0.80\n",
      "2003-12-16       0.99\n",
      "2003-12-23       1.30\n",
      "2003-12-30       1.32\n",
      "2004-01-06       1.71\n",
      "2004-01-13       2.17\n",
      "2004-01-20       2.82\n",
      "2004-01-27       3.67\n",
      "2004-02-03       4.40\n",
      "2004-02-10       4.83\n",
      "2004-02-17       4.75\n",
      "2004-02-24       4.62\n",
      "2004-03-02       4.00\n",
      "2004-03-09       3.56\n",
      "2004-03-16       3.29\n",
      "2004-03-23       2.74\n",
      "2004-03-30       2.15\n",
      "2004-04-06       1.68\n",
      "2004-04-13       1.20\n",
      "2004-04-20       0.77\n",
      "2004-04-27       0.60\n",
      "2004-05-04        NaN\n",
      "2004-05-11        NaN\n",
      "2004-05-18        NaN\n",
      "...               ...\n",
      "2015-09-29        NaN\n",
      "2015-10-06        NaN\n",
      "2015-10-13        NaN\n",
      "2015-10-20        NaN\n",
      "2015-10-27       0.50\n",
      "2015-11-03       0.68\n",
      "2015-11-10       0.76\n",
      "2015-11-17       0.88\n",
      "2015-11-24       1.02\n",
      "2015-12-01       1.16\n",
      "2015-12-08       1.21\n",
      "2015-12-15       1.45\n",
      "2015-12-22       1.66\n",
      "2015-12-29       2.11\n",
      "2016-01-05       4.26\n",
      "2016-01-12       2.76\n",
      "2016-01-19       3.62\n",
      "2016-01-26       4.73\n",
      "2016-02-02       5.47\n",
      "2016-02-09       6.06\n",
      "2016-02-16       5.90\n",
      "2016-02-23       6.14\n",
      "2016-03-01       5.88\n",
      "2016-03-08       5.83\n",
      "2016-03-15       5.00\n",
      "2016-03-22       4.23\n",
      "2016-03-29       3.18\n",
      "2016-04-05       2.30\n",
      "2016-04-12       1.85\n",
      "2016-04-19       1.19\n",
      "\n",
      "[652 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def importInflunet(path):\n",
    "    '''\n",
    "    Reads the Influnet data and creates a unique multiindex dataframe of the format\n",
    "    \n",
    "    (year,week) - incidence\n",
    "    \n",
    "    :param path: location of the influnet folder\n",
    "    :return: compacted version of \n",
    "    '''\n",
    "    parser = lambda d: dt.datetime.strptime(d + '-2', \"%Y-%W-%w\")\n",
    "    \n",
    "    df = pd.concat([pd.read_csv(path+t,\n",
    "            names=[\"time\",\"incidence\"], sep=\" \", parse_dates = [\"time\"], date_parser = parser, \n",
    "        header=1, usecols=[0,4], decimal=\",\") for t in listdir(path)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = df.set_index([\"time\"], append=False)\n",
    "    df[\"incidence\"] = df[\"incidence\"].astype(float)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    df = df.groupby(df.index).sum()\n",
    "    alpha = df.index[0]\n",
    "    omega = df.index[-1]\n",
    "    time_range = pd.date_range(alpha, omega, freq='W-TUE') #You have to state which is the lasy day of th week\n",
    "    df = df.reindex(time_range, fill_value= np.nan)\n",
    "    return df\n",
    "\n",
    "influnet_raw = importInflunet(\"/home/aalto/Desktop/DE/hw2/influnet/data/\")\n",
    "print influnet_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWikiRaw(wikiPages, path = \"/home/aalto/PycharmProjects/digitalepidemiology/data/\"):\n",
    "    '''\n",
    "    lplp\n",
    "    :param wikiPages: list of the wikipages that we want to analyze\n",
    "    :param path: location of the downloaded wikipedia pages\n",
    "    :return: \n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for wikiPage in wikiPages:\n",
    "        wiki = pd.read_csv(path+wikiPage+\".csv\", usecols=[0,1], parse_dates=True, index_col=[0], header=None)\n",
    "        wiki = wiki.resample(\"W\").sum()\n",
    "        df = df.reindex(wiki.index)\n",
    "        df[wikiPage] = wiki\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comparePlots(elems, start):\n",
    "    for elem in elems:\n",
    "        y = elem.ix[elem.index.year > start, 0]\n",
    "        y = y/max(y)\n",
    "        plt.plot(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getWiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5e47512b5d54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minflunet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportInflunet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/aalto/Desktop/DE/hw2/influnet/data/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetWiki\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"influenza2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomparePlots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minflunet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2010\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getWiki' is not defined"
     ]
    }
   ],
   "source": [
    "influnet = importInflunet(\"/home/aalto/Desktop/DE/hw2/influnet/data/\")\n",
    "wiki = getWiki([\"influenza2\"])\n",
    "\n",
    "comparePlots([influnet, wiki], 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getCrossCorrelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-51a6eb0ed3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetCrossCorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minflunet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2010\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2017\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'getCrossCorrelation' is not defined"
     ]
    }
   ],
   "source": [
    "getCrossCorrelation([influnet, wiki], range(2010,2017))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we need the wikipedia pages views count in order to compare them with influnet. In order to satisy the requirement for the bonus point I wrote a simple script that downloads the files from https://dumps.wikimedia.org/, scan it to find the words we are intrested in and writes those entries of the files that we are intrested and write them on different files. \n",
    "\n",
    "Once those files are collected and stored on our disk we can call this function which loads the element passed them in memoery and group them by week.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check the previous point we load the influnet dataset in memory and than show its plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 - Other functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are intrested to find if there is any other wikipedia pages that are able to gives us a better insghit about the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWikis(path = \"/home/aalto/PycharmProjects/digitalepidemiology/data/influenza1.csv\"):\n",
    "    '''\n",
    "    lplp\n",
    "    :param wikiPages: list of the wikipages that we want to analyze\n",
    "    :param path: location of the downloaded wikipedia pages\n",
    "    :return: \n",
    "    '''\n",
    "    df = pd.read_csv(path, parse_dates=['Date'])\n",
    "    df = df.set_index([\"Date\"], append=False)\n",
    "    del df[\"Week Number \"]\n",
    "    new_columns = dict((column,column[:-4].lower()) for column in df.columns.values)\n",
    "    df = df.rename(columns = new_columns )\n",
    "    del df[\"unname\"]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getCrossCorrelation(elems, years):\n",
    "    '''\n",
    "    :param elems: \n",
    "    :param years: \n",
    "    :return: \n",
    "    '''\n",
    "    y = len(years)\n",
    "    heatmap = pd.DataFrame(np.zeros(y**2).reshape(y,y), index = years, columns=years)\n",
    "    \n",
    "    for y1 in years:\n",
    "        for y2 in years:\n",
    "            #print(\"ciao\",elems[0][elems[0].index.year == year1])\n",
    "            a = elems[0][elems[0].index.year == y1]\n",
    "            a = a[\"incidence\"]/max(a[\"incidence\"])\n",
    "            b = elems[1][elems[1].index.year == y2]\n",
    "            b = b/max(b)\n",
    "            minimum = min(len(a), len(b))\n",
    "            heatmap.ix[y1,y2] = pearsonr(a[:minimum].values, b[:minimum].values)[0]\n",
    "    print heatmap\n",
    "    sb.heatmap(heatmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "influnet_raw = importInflunet(\"/home/aalto/Desktop/DE/hw2/influnet/data/\")\n",
    "influnet_raw = influnet_raw[(influnet_raw.index.year > 2007) & (influnet_raw.index.year < 2016)]\n",
    "\n",
    "\n",
    "wikis = getWikis()\n",
    "wikis = wikis[(wikis.index.year > 2007) & (wikis.index.year < 2016)]\n",
    "\n",
    "\n",
    "keep =  influnet_raw[\"incidence\"].notnull().values\n",
    "influnet = influnet_raw[keep]\n",
    "wikis = wikis[keep]\n",
    "\n",
    "\n",
    "\n",
    "#for wiki in wikis:\n",
    "    #plt.plot(influnet/max(influnet[\"incidence\"]))\n",
    "    #plt.plot(wikis[wiki]/max(wikis[wiki]))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikis = getWikis()\n",
    "np.array(wikis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Section\n",
    "\n",
    "In this section we are going to focus on flu prediction. To estimate flu seasonal outbreaks we used several regression models, that we trained using Wikipedia's pagegivews. We begin our analysis by starting with simple linear model as:\n",
    "\n",
    "* Linear Regression\n",
    "* Lasso \n",
    "* Elastic Net Regressor\n",
    "\n",
    "We than proceed in our analysis by tring different non linear regressors. For every model we are going to perform parameters selection to understand which parameters are more relevant in order to undestand and cross-validation to find the best parameters. The models are:\n",
    "\n",
    "* SVM\n",
    "* Deep Neural Networks\n",
    "\n",
    "After this first phase we are going to add to more parameters to our analys:\n",
    "\n",
    "* Influnet incidence for the week precdiing the target week\n",
    "* pageviews counts for all the pages that you selected for the week preceeding the target week\n",
    "\n",
    "and check how good performance increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1 Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use lasso. After all a Lasso is nothing else than a Elastic net with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. For this reason, the Lasso and its variants are fundamental to the field of compressed sensing. Under certain conditions, it can recover the exact set of non-zero weights (see Compressive sensing: tomography reconstruction with L1 prior (Lasso)).\n",
    "Mathematically, it consists of a linear model trained with \\ell_1 prior as regularizer. The objective function to minimize is:\n",
    "\n",
    "$\\underset{w}{min\\,} { \\frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}$\n",
    "\n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with \\alpha ||w||_1 added, where \\alpha is a constant and ||w||_1 is the \\ell_1-norm of the parameter vector.\n",
    "The implementation in the class Lasso uses coordinate descent as the algorithm to fit the coefficients. See Least Angle Regression for another implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def predictFluOutbreak(y):\n",
    "    lassoreg = linear_model.Lasso(alpha=0.1)\n",
    "    X = []\n",
    "    Y = influnet[influnet.year == y]\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        clf.fit(X_train, Y_train)\n",
    "        y_hat = lassoreg.predict(X_test)\n",
    "        rms = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrintPrediction(X_test, Y_test, Y_hat):\n",
    "    \n",
    "    alpha = X_test.index[0]\n",
    "    omega = X_test.index[-1]\n",
    "    time_range = pd.date_range(alpha, omega, freq='W-TUE')\n",
    "\n",
    "    Y_hat = Y_hat.reindex(time_range, fill_value= np.nan)\n",
    "    Y_test = Y_test.reindex(time_range, fill_value= np.nan)\n",
    "\n",
    "\n",
    "    plt.plot(Y_hat.index, Y_test['incidence'], linestyle='-', marker='o')\n",
    "    plt.plot(Y_hat.index, Y_hat,linestyle='-', marker='o')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LassoPrediction(X_train, Y_train, X_test, Y_test):\n",
    "    CVlassoreg = LassoCV(alphas = [.0001, .5, 10,1000,0.00000000001],eps = 1e-1000, cv=50, fit_intercept=False)\n",
    "    CVlassoreg.fit(X_train.values, Y_train['incidence'].values)\n",
    "    Y_hat = pd.DataFrame(CVlassoreg.predict(X_test.values), index= X_test.index)\n",
    "    print(len(X_test.index),len(Y_hat), len(Y_test['incidence'].values))\n",
    "    rms = sqrt(mean_squared_error(Y_test['incidence'].values, Y_hat))\n",
    "    PrintPrediction(X_test, Y_test, Y_hat)\n",
    "    return rms, CVlassoreg.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ElasticNetPrediction(X_train, Y_train, X_test, Y_test):\n",
    "    CVelasticreg = ElasticNetCV(l1_ratio=0.9, normalize=False,alphas = [.0001, .5, 10],eps = 1e-1000, cv=50, fit_intercept=False)\n",
    "    CVelasticreg.fit(X_train.values, Y_train['incidence'].values)\n",
    "    Y_hat = pd.DataFrame(CVelasticreg.predict(X_test.values), index= X_test.index)\n",
    "    print(len(X_test.index),len(Y_hat), len(Y_test['incidence'].values))\n",
    "    rms = sqrt(mean_squared_error(Y_test['incidence'].values, Y_hat))\n",
    "    PrintPrediction(X_test, Y_test, Y_hat)\n",
    "    return rms, CVelasticreg.get_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous function could have been done with one line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(27, 27, 27)\n",
      "(27, 27, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.649599455058159,\n",
       " {'alphas': [0.0001, 0.5, 10],\n",
       "  'copy_X': True,\n",
       "  'cv': 50,\n",
       "  'eps': 0.0,\n",
       "  'fit_intercept': False,\n",
       "  'l1_ratio': 0.9,\n",
       "  'max_iter': 1000,\n",
       "  'n_alphas': 100,\n",
       "  'n_jobs': 1,\n",
       "  'normalize': False,\n",
       "  'positive': False,\n",
       "  'precompute': 'auto',\n",
       "  'random_state': None,\n",
       "  'selection': 'cyclic',\n",
       "  'tol': 0.0001,\n",
       "  'verbose': 0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "print \n",
    "X_train,Y_train = wikis[(wikis.index.year > 2007) & (wikis.index.year < 2015)], influnet[(influnet.index.year > 2007) & (influnet.index.year < 2015)]  \n",
    "X_test, Y_test  = wikis[(wikis.index.year == 2015)], influnet[(influnet.index.year == 2015)]\n",
    "\n",
    "LassoPrediction(X_train, Y_train, X_test, Y_test)\n",
    "ElasticNetPrediction(X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now intrested in building a better estimation for our model, so we will use in this case two more features.\n",
    "\n",
    "* Incidence for the previous week\n",
    "* Pages contus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 27, 27)\n",
      "(27, 27, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.010117646039902,\n",
       " {'alphas': [0.0001, 0.5, 10],\n",
       "  'copy_X': True,\n",
       "  'cv': 50,\n",
       "  'eps': 0.0,\n",
       "  'fit_intercept': False,\n",
       "  'l1_ratio': 0.9,\n",
       "  'max_iter': 1000,\n",
       "  'n_alphas': 100,\n",
       "  'n_jobs': 1,\n",
       "  'normalize': False,\n",
       "  'positive': False,\n",
       "  'precompute': 'auto',\n",
       "  'random_state': None,\n",
       "  'selection': 'cyclic',\n",
       "  'tol': 0.0001,\n",
       "  'verbose': 0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tWeek = X_test.index[-1]\n",
    "\n",
    "wikis[\"incidence\"] =  influnet[\"incidence\"]\n",
    "Y_predict = wikis[\"incidence\"][wikis[\"incidence\"].index.date == tWeek]\n",
    "wikis[\"incidence\"] = wikis[\"incidence\"].shift(1)\n",
    "\n",
    "wikis = wikis.drop(wikis.index[0]) #remove first row\n",
    "\n",
    "X_train,Y_train = wikis[(wikis.index.year > 2008) & (wikis.index.year < 2015)], influnet[(influnet.index.year > 2008) & (influnet.index.year < 2015)]  \n",
    "X_test, Y_test  = wikis[(wikis.index.year == 2015)], influnet[(influnet.index.year == 2015)]\n",
    "\n",
    "LassoPrediction(X_train, Y_train, X_test, Y_test)\n",
    "ElasticNetPrediction(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train.index, influnet[\"incidence\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Features Enanchements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def padInflunet(aux, year):\n",
    "    '''\n",
    "    The influnet dataset lacks information about the weeks that do not belog to the flu season (usally, but not necessarly, from week 17 to 40).\n",
    "    This functions fills the dataset with empty position in order to match the wikipedia format.\n",
    "    \n",
    "    :param aux: Influnet dataframe from a specific year\n",
    "    :param year: year of the previous Influnet dataframe\n",
    "    :return: padded version of the original dataframe\n",
    "    '''\n",
    "    year_weeks = aux.index.values[-1]\n",
    "    week_range = range(1,year_weeks+1)\n",
    "    aux = aux.reindex(week_range, fill_value=0)\n",
    "    aux[\"year\"] = year\n",
    "    aux[\"week\"] = week_range\n",
    "    \n",
    "    aux.set_index(['year', 'week'], append=False, inplace=True)\n",
    "    return aux\n",
    "\n",
    "\n",
    "def getInflunet(path = \"/home/aalto/Desktop/DE/hw2/influnet/data/\"):\n",
    "    '''\n",
    "    import and reformat the original Influnet dataset\n",
    "    \n",
    "    :param path: \n",
    "    :return: clean and padded version of the Influnet dataset\n",
    "    '''\n",
    "    \n",
    "    df = importInflunet();\n",
    "    previous = None\n",
    "    for x,y in df.index.values:\n",
    "        if previous == None:\n",
    "            df2 = reindexDF(df.loc[x], x)\n",
    "        elif x != previous:\n",
    "            df2 = df2.append(reindexDF(df.loc[x], x))\n",
    "        previous = x\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
